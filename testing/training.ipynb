{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of the loaded object: <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Replace 'your_file.pkl' with the actual path to your Pickle file\n",
    "file_path = 'texts.pkl'\n",
    "\n",
    "# Open the Pickle file in binary read mode\n",
    "with open(file_path, 'rb') as file:\n",
    "    try:\n",
    "        # Load the object from the Pickle file\n",
    "        your_object = pickle.load(file)\n",
    "\n",
    "        # Print information about the loaded object\n",
    "        print(\"Type of the loaded object:\", type(your_object))\n",
    "\n",
    "        # If it's a dictionary, print keys\n",
    "        if isinstance(your_object, dict):\n",
    "            print(\"Keys in the dictionary:\", list(your_object.keys()))\n",
    "\n",
    "        # You can add more type-specific checks or print statements based on the expected structure\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error loading the Pickle file:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys at the root level: ['model_weights', 'optimizer_weights']\n",
      "Group: model_weights\n",
      "  Keys in model_weights: ['dense', 'dense_1', 'dense_2', 'dropout', 'dropout_1', 'top_level_model_weights']\n",
      "Group: optimizer_weights\n",
      "  Keys in optimizer_weights: ['SGD']\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "# Replace 'your_file.h5' with the actual path to your HDF5 file\n",
    "file_path = 'model.h5'\n",
    "\n",
    "# Open the HDF5 file in read-only mode\n",
    "with h5py.File(file_path, 'r') as file:\n",
    "    # Print keys at the root level\n",
    "    print(\"Keys at the root level:\", list(file.keys()))\n",
    "\n",
    "    # Iterate through keys and print information about groups and datasets\n",
    "    for key in file.keys():\n",
    "        if isinstance(file[key], h5py.Group):\n",
    "            print(f\"Group: {key}\")\n",
    "            # Print keys within the group\n",
    "            print(f\"  Keys in {key}: {list(file[key].keys())}\")\n",
    "        elif isinstance(file[key], h5py.Dataset):\n",
    "            print(f\"Dataset: {key}\")\n",
    "            # Print information about the dataset\n",
    "            print(f\"  Shape: {file[key].shape}\")\n",
    "            print(f\"  Datatype: {file[key].dtype}\")\n",
    "            # You can choose to print a subset of the data if the dataset is not too large\n",
    "            print(f\"  Data (partial): {file[key][:]}\")\n",
    "        else:\n",
    "            print(f\"Unknown type for key: {key}\")\n",
    "\n",
    "# The file is automatically closed when you exit the 'with' block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "286 documents\n",
      "129 classes ['What are the types of depression?', 'about', 'afternoon', 'anxious', 'ask', 'at what age does anxiety peak?', 'can lack of sleep make you feel sad?', 'can low blood sugar cause suicidal thoughts?', 'casual', 'creation', 'death', 'default', 'depressed', 'do we control our thoughts?', 'does oversleeping cause depression?', 'done', 'evening', 'fact-1', 'fact-10', 'fact-11', 'fact-12', 'fact-13', 'fact-14', 'fact-15', 'fact-16', 'fact-17', 'fact-18', 'fact-19', 'fact-2', 'fact-20', 'fact-21', 'fact-22', 'fact-23', 'fact-24', 'fact-25', 'fact-26', 'fact-27', 'fact-28', 'fact-29', 'fact-3', 'fact-30', 'fact-31', 'fact-32', 'fact-5', 'fact-6', 'fact-7', 'fact-8', 'fact-9', 'friends', 'goodbye', 'greeting', 'happy', 'hate-me', 'hate-you', 'help', 'how can we reduce anxiety?', 'how does depression affect the world?', 'how long can anxiety last?', 'how many thoughts a day do we have?', 'i am a victim of bullying', 'i am afraid i will fail again', 'i am afraid to file a case against bullying', 'i am feeling anxious lately.', 'i am feeling stressed lately', 'i am good for nothing!', 'i am good for nothing.', 'i am lonely!', 'i am sad', 'i am stressed out', \"i can't do this anymore\", 'i feel i have let my parents down', 'i hate losing.', 'i hate myself!', 'i let everyojokne down', 'i think i am ugly!', \"i think i'm losing my mind\", 'i want a break', 'i want to kill myself', 'i want to leave the cou ntry and run away', 'i will never succeed in life', \"i wish i could've been a winner\", 'i wish i was better than them', 'i wish to quit', 'is depression a side effect of diabetes?', 'is school a cause of depression?', 'jokes', 'learn-mental-health', 'learn-more', 'location', 'meditation', 'mental-health-fact', 'morning', 'my time has come', 'neutral-response', 'night', 'no one likes me!', 'no-approach', 'no-response', 'not-talking', 'problem', 'repeat', 'sad', 'scared', 'skill', 'sleep', 'something-else', 'stressed', 'stupid', 'suicide', 'thanks', 'thearpy-useful', 'understand', 'user-advice', 'user-agree', 'user-meditation', 'what are the causes of depression?', 'what are the stages of anxiety?', 'what are the top causes of depression?', 'what is depression?', 'what is the 3 3 3 rule for anxiety?', 'what is the biological cause of depression?', 'what is the meaning of anxiety and depression?', 'which age group has the highest rate of depression?', 'which country has the highest rate of depression?', 'which country has the lowest rate of depression?', 'which race has the highest rate of depression?', 'why is anxiety bad for you?', 'worthless', 'wrong']\n",
      "356 unique lemmatized words [\"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", ',', '.', '3', 'a', 'about', 'absolutely', 'advice', 'affect', 'afraid', 'afternoon', 'again', 'against', 'age', 'all', 'alot', 'already', 'am', 'and', 'another', 'answer', 'anxiety', 'anxious', 'any', 'anymore', 'anyone', 'anything', 'appears', 'approaching', 'are', 'ask', 'at', 'au', 'available', 'away', 'awful', 'bad', 'bb', 'be', 'because', 'become', 'been', 'before', 'better', 'between', 'biological', 'blood', 'bonjour', 'boyfriend', 'break', 'bring', 'brother', 'bullying', 'burned', 'bye', 'ca', 'call', 'can', 'case', 'cause', 'cheerful', 'child', 'come', 'commit', 'connection', 'continue', 'control', 'cou', 'could', 'country', 'crazy', 'created', 'cure', 'dad', 'day', 'define', 'depressed', 'depression', 'deserve', 'diabetes', 'did', 'die', 'died', 'difference', 'different', 'disorder', 'do', 'doe', 'down', 'dumb', 'effect', 'else', 'empty', 'enough', 'evening', 'everyojokne', 'exam', 'fact', 'fail', 'family', 'fare', 'feel', 'feeling', 'few', 'file', 'financial', 'find', 'fine', 'focus', 'for', 'friend', 'from', 'get', 'girlfriend', 'give', 'go', 'going', 'good', 'goodbye', 'great', 'group', 'guess', 'ha', 'had', 'hand', 'happy', 'hate', 'have', 'hay', 'health', 'hello', 'help', 'helpful', 'hey', 'hi', 'highest', 'hmmm', 'hola', 'how', 'howdy', 'i', 'if', 'ill', 'illness', 'importance', 'important', 'in', 'insominia', 'insomnia', 'interested', 'involved', 'is', 'it', 'joke', 'just', 'k', 'kill', 'killing', 'know', 'lack', 'last', 'lately', 'later', 'learn', 'learning', 'leave', 'let', 'life', 'like', 'live', 'location', 'lonely', 'long', 'losing', 'low', 'lowest', 'made', 'maintain', 'make', 'many', 'me', 'mean', 'meaning', 'medication', 'meditation', 'mental', 'mentally', 'mentioned', 'mind', 'mom', 'money', 'more', 'morning', 'much', 'my', 'myself', \"n't\", 'name', 'need', 'never', 'new', 'nice', 'night', 'no', 'nobody', 'not', 'nothing', 'now', 'ntry', 'of', 'oh', 'ok', 'okay', 'on', 'one', 'open', 'option', 'or', 'our', 'out', 'oversleeping', 'parent', 'passed', 'past', 'peak', 'people', 'please', 'possibly', 'practicing', 'prepared', 'prevent', 'probably', 'problem', 'professional', 'proper', 'quit', 'race', 'rate', 'really', 'recover', 'reduce', 'relationship', 'repeating', 'response', 'revoir', 'right', 'robot', 'rule', 'run', 'sad', 'sadness', 'said', 'sasa', 'say', 'saying', 'sayonara', 'scared', 'school', 'see', 'seem', 'sense', 'should', 'shut', 'side', 'sign', 'sister', 'sleep', 'slept', 'so', 'social', 'some', 'someone', 'something', 'sound', 'stage', 'starting', 'stay', 'still', 'stress', 'stressed', 'stuck', 'stupid', 'succeed', 'suffering', 'sugar', 'suicidal', 'suicide', 'support', 'sure', 'symptom', 'take', 'talk', 'tell', 'than', 'thank', 'thanks', 'that', 'the', 'thee', 'them', 'then', 'therapist', 'therapy', 'there', 'think', 'this', 'thought', 'through', 'time', 'to', 'today', 'told', 'top', 'treatment', 'trust', 'type', 'ugly', 'understand', 'understands', 'unwell', 'up', 'useful', 'useless', 'very', 'victim', 'wa', 'want', 'warning', 'way', 'we', 'well', 'were', 'what', 'whatever', 'where', 'which', 'who', 'why', 'will', 'winner', 'wish', 'with', 'world', 'worried', 'worthless', 'would', 'wrong', 'yeah', 'yes', 'you', 'your', 'yourself']\n"
     ]
    }
   ],
   "source": [
    "# tokenizing and lematizing\n",
    "words=[]\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_words = ['?', '!']\n",
    "data_file = open('intents.json').read()\n",
    "intents = json.loads(data_file)\n",
    "\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        #tokenize each word\n",
    "        w = nltk.word_tokenize(pattern)\n",
    "        words.extend(w)\n",
    "        #add documents in the corpus\n",
    "        documents.append((w, intent['tag']))\n",
    "        # add to our classes list\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])\n",
    "# lemmatize and lower each word and remove duplicates\n",
    "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n",
    "words = sorted(list(set(words)))\n",
    "# sort classes\n",
    "classes = sorted(list(set(classes)))\n",
    "# documents = combination between patterns and intents\n",
    "print (len(documents), \"documents\")\n",
    "# classes = intents\n",
    "print (len(classes), \"classes\", classes)\n",
    "# words = all words, vocabulary\n",
    "print (len(words), \"unique lemmatized words\", words)\n",
    "pickle.dump(words,open('texts.pkl','wb'))\n",
    "pickle.dump(classes,open('labels.pkl','wb'))\n",
    "# create our training data\n",
    "training = []\n",
    "# create an empty array for our output\n",
    "output_empty = [0] * len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# training set, bag of words for each sentence\n",
    "for doc in documents:\n",
    "    # initialize our bag of words\n",
    "    bag = []\n",
    "    # list of tokenized words for the pattern\n",
    "    pattern_words = doc[0]\n",
    "    # lemmatize each word - create base word, in attempt to represent related words\n",
    "    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n",
    "    # create our bag of words array with 1, if word match found in current pattern\n",
    "    for w in words:\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "    \n",
    "    # output is a '0' for each tag and '1' for current tag (for each pattern)\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "    \n",
    "    training.append([bag, output_row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (286, 2) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# shuffle our features and turn into np.array\u001b[39;00m\n\u001b[1;32m      2\u001b[0m random\u001b[38;5;241m.\u001b[39mshuffle(training)\n\u001b[0;32m----> 3\u001b[0m training \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# create train and test lists. X - patterns, Y - intents\u001b[39;00m\n\u001b[1;32m      5\u001b[0m train_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(training[:,\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (286, 2) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "\n",
    "# shuffle our features and turn into np.array\n",
    "random.shuffle(training)\n",
    "training = np.array(training)\n",
    "# create train and test lists. X - patterns, Y - intents\n",
    "train_x = list(training[:,0])\n",
    "train_y = list(training[:,1])\n",
    "print(\"Training data created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create model - 3 layers. First layer 128 neurons, second layer 64 neurons and 3rd output layer contains number of neurons\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# equal to number of intents to predict output intent with softmax\u001b[39;00m\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential()\n\u001b[0;32m----> 4\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dense(\u001b[38;5;241m128\u001b[39m, input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mtrain_x\u001b[49m[\u001b[38;5;241m0\u001b[39m]),), activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dropout(\u001b[38;5;241m0.5\u001b[39m))\n\u001b[1;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dense(\u001b[38;5;241m64\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_x' is not defined"
     ]
    }
   ],
   "source": [
    "# Create model - 3 layers. First layer 128 neurons, second layer 64 neurons and 3rd output layer contains number of neurons\n",
    "# equal to number of intents to predict output intent with softmax\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(train_y[0]), activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model. Stochastic gradient descent with Nesterov accelerated gradient gives good results for this model\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#fitting and saving the model \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m hist \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(np\u001b[38;5;241m.\u001b[39marray(\u001b[43mtrain_x\u001b[49m), np\u001b[38;5;241m.\u001b[39marray(train_y), epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel.h5\u001b[39m\u001b[38;5;124m'\u001b[39m, hist)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel created\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_x' is not defined"
     ]
    }
   ],
   "source": [
    "#fitting and saving the model \n",
    "hist = model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=5, verbose=1)\n",
    "model.save('model.h5', hist)\n",
    "print(\"model created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
